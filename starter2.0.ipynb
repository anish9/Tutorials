{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learn_work_tf2.2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdfKFkd1fLXN",
        "colab_type": "code",
        "outputId": "0d2a5261-6800-4ad7-8eb5-dba20dc94071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import cv2\n",
        "%matplotlib inline\n",
        "print(tf.__version__)\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import  MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# drive.mount(\"gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n",
            "Drive already mounted at gdrive; to attempt to forcibly remount, call drive.mount(\"gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_nb3OyiUf0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"sample_data/california_housing_train.csv\")\n",
        "\n",
        "data.head()\n",
        "x = data.iloc[:,2:-1]\n",
        "y = data.iloc[:,-1]\n",
        "x = x.values\n",
        "y = y.values\n",
        "# xtr,ytr,xte,yte = train_test_split(x,y,test_size=0.2,shuffle=True)\n",
        "\n",
        "ms = MinMaxScaler()\n",
        "xtrain = ms.fit_transform(x)\n",
        "ytrain = ms.fit_transform(y.reshape(-1,1))\n",
        "\n",
        "mshape = xtrain.shape[0]\n",
        "batchsize = 128\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((xtrain,ytrain))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batchsize).repeat()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUxwZmE728H7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim  = 8\n",
        "hidden_size1 = 56\n",
        "hidden_size2 = 56\n",
        "output_dim = 1\n",
        "\n",
        "inp_h1_w = tf.Variable(tf.random.normal(shape=(input_dim,hidden_size1)))\n",
        "inp_h1_b = tf.Variable(tf.zeros((hidden_size1,)))\n",
        "h1_h2_w  = tf.Variable(tf.random.normal(shape=(hidden_size1,hidden_size2)))\n",
        "h1_h2_b  = tf.Variable(tf.zeros((hidden_size2,)))\n",
        "h2_o_w   = tf.Variable(tf.random.normal(shape=(hidden_size2,output_dim)))\n",
        "h2_o_b   = tf.Variable(tf.zeros((output_dim,)))\n",
        "\n",
        "def regress(x):\n",
        "  x  = tf.cast(x,tf.float32)\n",
        "  comp1 = tf.matmul(x,inp_h1_w)+inp_h1_b\n",
        "  comp1 = Activation(\"relu\")(comp1)\n",
        "  comp2 = tf.matmul(comp1,h1_h2_w)+h1_h2_b\n",
        "  comp2 = Activation(\"relu\")(comp2)\n",
        "  comp3 = tf.matmul(comp2,h2_o_w)+h2_o_b\n",
        "  comp3 = Activation(\"sigmoid\")(comp3)\n",
        "  return comp3\n",
        "\n",
        "\n",
        "def compute_loss(labels, predictions):\n",
        "  labels = tf.cast(labels,tf.float32)\n",
        "  return tf.reduce_mean(tf.square(labels - predictions))\n",
        "\n",
        "learning_rate = 0.00001\n",
        "\n",
        "def train_on_batch(x,y):\n",
        "  with tf.GradientTape(persistent=True) as t:\n",
        "    func_out  = regress(x)\n",
        "    loss  = compute_loss(y,func_out)\n",
        "    if loss > 0.5:\n",
        "      t.reset()\n",
        "    dl_o,db_o   = t.gradient(loss,[h2_o_w,h2_o_b])\n",
        "    dl_h2,db_h2 = t.gradient(loss,[h1_h2_w,h1_h2_b])\n",
        "    dl_h1,db_h1 = t.gradient(loss,[inp_h1_w,inp_h1_b])\n",
        "  h2_o_w.assign_sub(learning_rate * dl_o)\n",
        "  h2_o_b.assign_sub(learning_rate * db_o)\n",
        "  h1_h2_w.assign_sub(learning_rate*dl_h2)\n",
        "  h1_h2_b.assign_sub(learning_rate*db_h2)\n",
        "  inp_h1_w.assign_sub(learning_rate*dl_h1)\n",
        "  inp_h1_b.assign_sub(learning_rate*db_h1)\n",
        "  return tf.reduce_sum(loss)\n",
        "\n",
        "for epoch in range(40):\n",
        "  for step, (x, y) in enumerate(dataset):\n",
        "    loss = train_on_batch(x, y)\n",
        "    print(\"Epoch %d : loss = %.4f\" %(step,loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgrUD4-nfTVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class NN(Model):\n",
        "  def __init__(self):\n",
        "    super(NN,self).__init__()\n",
        "    self.inp = InputLayer(input_shape=(6,))\n",
        "    self.l1 = Dense(128,activation=\"relu\")\n",
        "    self.l2 = Dense(128,activation=\"relu\")\n",
        "    self.d = Dropout(0.1)\n",
        "    self.b = BatchNormalization(axis=-1)\n",
        "    self.out = Dense(1,activation=\"sigmoid\")\n",
        "\n",
        "  def call(self,inputs):\n",
        "    x = self.inp(inputs)\n",
        "    x = self.l1(x)\n",
        "    x = self.d(x)\n",
        "    x = self.b(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.b(x)\n",
        "    x = self.l2(x)\n",
        "    o = self.out(x)\n",
        "    return o\n",
        "\n",
        "\n",
        "\n",
        "def NN():\n",
        "  inp = Input(shape=(6,))\n",
        "  d1 = Dense(64,activation=\"relu\")(inp)\n",
        "  b1= BatchNormalization()(d1)\n",
        "  d2 = Dense(128,activation=\"relu\")(b1)\n",
        "  fp = Dropout(0.1)(d2)\n",
        "  o  = Dense(1,activation=\"sigmoid\")(fp)\n",
        "  mod = Model(inp,o)\n",
        "  return mod\n",
        "\n",
        "  # dec.fit(dataset,epochs=30,steps_per_epoch=mshape//batchsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWffKKU7f1RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optim = Nadam(1e-3)\n",
        "build = NN()\n",
        "\n",
        "def train_batch(x,y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    pred = build(x)\n",
        "    loss = tf.keras.losses.mean_squared_error(y,pred)\n",
        "    grad = tape.gradient(loss,build.trainable_variables)\n",
        "    optim.apply_gradients(zip(grad,build.trainable_variables))\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "for j in range(1):\n",
        "  for s,(x,y) in enumerate(dataset):\n",
        "    outs = train_batch(x,y)\n",
        "    print(\"epochs - %d | loss : %.4f\"%(s,outs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FvATBr5gMoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq3_pMgSgOBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2eJTp3QhIhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "533jYwrak0XD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjBsdhslk8Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}